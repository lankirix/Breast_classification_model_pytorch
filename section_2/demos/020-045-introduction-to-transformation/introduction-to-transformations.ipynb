{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Transformations\n",
    "This demo covers Tranformations using PyTorch when dealing with image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Why Transformations Review\n",
    "In PyTorch, **transforms** are primarily used for **data preprocessing** and **augmentation**. They are essential in preparing data for deep learning models by applying various operations to the dataset, which can improve the efficiency and performance of a model. PyTorch's `torchvision.transforms` module provides a convenient way to apply these operations on image data.\n",
    "\n",
    "Here’s why PyTorch transforms are important:\n",
    "\n",
    "### 1. **Data Preprocessing**\n",
    "Transforms help convert raw data (e.g., images) into a format that can be fed into a model. For instance, images need to be converted to PyTorch tensors, resized to the required dimensions, or normalized to a certain range of values.\n",
    "\n",
    "\n",
    "### 2. **Data Augmentation**\n",
    "Data augmentation helps improve the model’s generalization by artificially increasing the size of the training dataset. By applying random transformations (like flipping, rotating, or cropping), models can learn to generalize better and avoid overfitting.\n",
    "\n",
    "\n",
    "### 3. **Improve Model Accuracy**\n",
    "By augmenting the data with random transformations, the model becomes more robust to variations in the input data. For example, a model trained on both original and rotated images can handle different orientations of objects during inference.\n",
    "\n",
    "### 4. **Pipelines**\n",
    "Transforms can be easily chained together using `transforms.Compose()`. This allows for a flexible, reusable pipeline where multiple transformations are applied in sequence. For instance, you can resize an image, convert it to a tensor, and then normalize it with one `Compose` call.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "PyTorch transforms simplify preprocessing data to the right format for the model, augmenting data to create diversity in training, and improving model robustness. They simplify the data preparation process and are essential tools in computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "The ones you see here are ones that could potentially be good for building image classification models.\n",
    "\n",
    "Please be sure to try some out on your own and/or read the documentation for other available transformations.\n",
    "\n",
    "https://pytorch.org/vision/stable/transforms.html#v2-api-reference-recommended "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_images(original_image, new_image=None, new_image_name=None):\n",
    "    \"\"\"\n",
    "    Helper function to display to images\n",
    "    \"\"\"\n",
    "    if new_image is not None:\n",
    "        # Create a figure with 1 row and 2 columns\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        # Display the original image\n",
    "        axes[0].imshow(original_image)\n",
    "        axes[0].axis('on')  # Turn off the axis for cleaner display\n",
    "        axes[0].set_title('Original Image')\n",
    "        # Display the new image\n",
    "        axes[1].imshow(new_image)\n",
    "        axes[1].axis('on')  # Turn off the axis for cleaner display\n",
    "        axes[1].set_title(new_image_name)\n",
    "    else:\n",
    "        # Create a single row figure\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "        # Display only a single image\n",
    "        plt.imshow(original_image)\n",
    "        plt.axis('on')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Getting started with Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import v2 of transforms\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import v1 of transforms\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an image\n",
    "from PIL import Image\n",
    "\n",
    "original_image = Image.open('images/cat/cat-1.jpg')\n",
    "print(original_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show image using our helper function\n",
    "display_images(original_image=original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize()\n",
    "Resize an image in pixels. This would ensure that all images are being presented in the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Using the Resize the image to 50 x 25 pixels using Resize()\n",
    "resize_transform = v2.Resize((50, 25))\n",
    "# Transform it\n",
    "resized_image = resize_transform(original_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show new image resize\n",
    "display_images(original_image=original_image, new_image=resized_image, new_image_name=\"Resized Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try again with v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomHorizontalFlip()\n",
    "Horizontally flip the input with a given probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a Random Horizontal Flip augmentation to a probability of 1 (100%)\n",
    "rhf_transform = v2.RandomHorizontalFlip(p=1)\n",
    "# Tranform it\n",
    "rhf_image = rhf_transform(original_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the transform\n",
    "display_images(original_image=original_image, new_image=rhf_image, new_image_name=\"Random Horizontal Flip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## ToTensor()\n",
    "Simply transform an image into a Tensor. This is necessary format for training a PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformation using ToTensor()\n",
    "tensor_tranform = v2.ToTensor()\n",
    "# Apply it to our original image\n",
    "tensor_image = tensor_tranform(original_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the original image format and the new\n",
    "print(f\"Original Image: {original_image}\")\n",
    "print(f\"Tensor Image: \\n{tensor_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize()\n",
    "Normalize a tensor image or video with mean and standard deviation.\n",
    "\n",
    "**NOTE**: This will NOT work on a PIL image (original_image). It must be in a tensor format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our tensor image above, lets normalize it\n",
    "normalize_transform = v2.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# Apply using the tensor_image above\n",
    "normalized_image = normalize_transform(tensor_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the normalized tensor and the original tensor\n",
    "print(normalized_image)\n",
    "print(tensor_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What just happened?\n",
    "\n",
    "The pixel values of images are typically in the range [0, 1] if they are converted to tensors using ToTensor() or [0, 255] in their raw form. This normalization effectively scales these values into a range centered around zero.\n",
    "\n",
    "This will:\n",
    "\n",
    "    Subtract 0.5 from each pixel value, making the range roughly centered around zero.\n",
    "    Divide by 0.5, which scales the pixel values to a range of [-1, 1].\n",
    "\n",
    "#### Why normalize to this range?\n",
    "\n",
    "Normalizing the pixel values to a standard range like [-1, 1] (or another centered range) helps deep learning models learn more efficiently because it standardizes the input data. It often leads to faster convergence and improved model performance.\n",
    "\n",
    "#### Tuples explained\n",
    "The first tuple (0.5, 0.5, 0.5) represents the mean values for each RGB channel.\n",
    "\n",
    "The second tuple (0.5, 0.5, 0.5) represents the standard deviation values for each RGB channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomCrop()\n",
    "Crop the input at a random location. This is not a probability but random location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly crop the image at a random location \n",
    "rc_transform = v2.RandomCrop(size=(100, 100))\n",
    "# Run the random crop\n",
    "rc_image = rc_transform(original_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the transform\n",
    "display_images(original_image=original_image, new_image=rc_image, new_image_name=\"Random Crop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run it a couple of times to see that it changes locations each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomPhotometricDistort()\n",
    "Randomly distorts the image or video. Adjust the contrast, saturation, hue, brightness, and also randomly permutes channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Distort. Notice the different settings.\n",
    "rpd_transform = v2.RandomPhotometricDistort(\n",
    "    brightness=(0.875, 1.125), \n",
    "    contrast=(0.5, 1.5), \n",
    "    saturation=(0.5, 1.5),\n",
    "    hue=(- 0.05, 0.05), \n",
    "    p=1\n",
    ")\n",
    "\n",
    "# Create the augmentation\n",
    "rpd_image = rpd_transform(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the differences\n",
    "display_images(original_image=original_image, new_image=rpd_image, new_image_name=\"Random Photometric Distort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run it several times to see the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomResize()\n",
    "Randomly resize the input. This may or may not be a good option keep in mind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transform\n",
    "rr_transform = v2.RandomResize(min_size=100, max_size=200)\n",
    "# Apply the augmentation\n",
    "rr_image = rr_transform(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the differences\n",
    "display_images(original_image=original_image, new_image=rr_image, new_image_name=\"Random Resize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines with Compose()\n",
    "Compose() allows for use to build a pipeline of transformations on images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build a pipeline using the above transformations. We define this in a list.\n",
    "transforms_pipeline = v2.Compose([\n",
    "    v2.RandomCrop(size=(100, 100)),\n",
    "    v2.RandomPhotometricDistort(\n",
    "        brightness=(0.875, 1.125), \n",
    "        contrast=(0.5, 1.5), \n",
    "        saturation=(0.5, 1.5),\n",
    "        hue=(- 0.05, 0.05), \n",
    "        p=1),\n",
    "    v2.RandomResize(min_size=75, max_size=150),\n",
    "    v2.RandomHorizontalFlip(p=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pipeline to our original image\n",
    "pipeline_image = transforms_pipeline(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the original to the new\n",
    "display_images(original_image=original_image, new_image=pipeline_image, new_image_name=\"Pipeline Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating with Datasets\n",
    "Lets load the FashionMNIST preloaded Dataset from the previous section and see how we can apply our transform pipeline to those images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the torchvision datasets library\n",
    "import torchvision.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset first without our pipeline\n",
    "original_image_dataset = torchvision.datasets.FashionMNIST(root='./fashion', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an image from the dataset\n",
    "original_image, label = original_image_dataset[2]\n",
    "# View it\n",
    "display_images(original_image=original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pipeline to fit the dataset\n",
    "transforms_pipeline = v2.Compose([\n",
    "    v2.RandomCrop(size=(15, 15)), # Adjust the crop to fit the image size\n",
    "    v2.RandomPhotometricDistort(\n",
    "        brightness=(0.875, 1.125), \n",
    "        contrast=(0.5, 1.5), \n",
    "        saturation=(0.5, 1.5),\n",
    "        hue=(- 0.05, 0.05), \n",
    "        p=1),\n",
    "    v2.RandomResize(min_size=10, max_size=15), # Adjust the crop to fit the image size\n",
    "    v2.RandomHorizontalFlip(p=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new datset with our pipeline\n",
    "pipeline_image_dataset = torchvision.datasets.FashionMNIST(root='./fashion', train=False, download=True, \n",
    "                                                           transform=transforms_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the same image from above but from the new dataset\n",
    "pipeline_image, label = pipeline_image_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the images together\n",
    "display_images(original_image=original_image, new_image=pipeline_image, new_image_name=\"Pipeline Image\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
