{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "PyTorch provides several methods for saving and loading models.\n",
    "\n",
    "This Demo will cover several methods using an example Model. \n",
    "\n",
    "#### Functions for Saving and Loading \n",
    "`torch.save()`: Save PyTorch objects (models, tensors, dictionaries, etc...) using Pythons pickle module.\n",
    "\n",
    "`torch.load()`: Loads PyTorch objects into memory.\n",
    "\n",
    "`load_state_dict()`: Loads saved parameters from objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Fake Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FakeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FakeNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.batch_norm = nn.BatchNorm1d(50) \n",
    "        self.fc2 = nn.Linear(50, 1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))              \n",
    "        x = self.batch_norm(x)               \n",
    "        x = self.fc2(x)                      \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our model\n",
    "model = FakeNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FakeDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate random input data with 10 features\n",
    "        x = torch.randn(10)\n",
    "        # Generate a random target value\n",
    "        y = torch.randn(1)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n",
    "# Create a dataset and data loader\n",
    "dataset = FakeDataset(num_samples=1000)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a fake model\n",
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading using `state_dict`\n",
    "`state_dict` is a dictionary that stores all the learnable parameters of a model, like weights and biases as well as hyperparameters of an Optimizer. This makes it easy to save, load, and transfer the modelâ€™s parameters, allowing flexible model saving and reloading across different environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the state_dict of the model\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the paramters of each layer\n",
    "for k, v in model.state_dict().items():\n",
    "    print(f\"Layer Name: {k} Parameters:{v.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the hyperparameters of the Optimizer\n",
    "print(optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the state_dict for each (recommended approach)\n",
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), \"model_state_dict.pt\") # .pt or pth extension for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the state_dict for the Optimizer\n",
    "torch.save(optimizer.state_dict(), \"optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: state_dict is ONLY saving the parameters!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference\n",
    "REVIEW: Inference is the process of using a trained model to make predictions.\n",
    "\n",
    "Let's load a model using using its state_dict and prepare it for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "new_model = FakeNet()\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the current state_dict\n",
    "for k, v in new_model.state_dict().items():\n",
    "    print(f\"Layer Name: {k} Parameters:{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parameters into our model\n",
    "new_model.load_state_dict(torch.load(\"model_state_dict.pt\", weights_only=True)) # ONLY the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print it again to show the difference\n",
    "for k, v in new_model.state_dict().items():\n",
    "    print(f\"Layer Name: {k} Parameters:{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters have been updated after loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example input\n",
    "import torch\n",
    "# Random batch size of 1-10 features\n",
    "sample_input = torch.randn(1, 10)\n",
    "print(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do an example infernce on our model\n",
    "new_model.eval()\n",
    "\n",
    "# Call the model with input to get a prediction\n",
    "output = new_model(sample_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading entire Model\n",
    "PyTorch provides the option to save a full model to the filesystem as well.\n",
    "\n",
    "full model = full python pickle version of model\n",
    "\n",
    "This can potentially cause issues because it relies on the exact class definitions and file structure from when the model was saved, so loading may fail if used in a different project or after code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save a full model\n",
    "import torch\n",
    "\n",
    "torch.save(model, \"model_full.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model class from file\n",
    "from fake_net import FakeNet\n",
    "\n",
    "# Initialize and use the model\n",
    "model = FakeNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try again\n",
    "torch.save(model, \"model_full.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the size difference\n",
    "!ls -lh model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a full model\n",
    "from fake_net import FakeNet\n",
    "\n",
    "# Initialize and use the model\n",
    "new_model = FakeNet()\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it from the full model\n",
    "new_model = torch.load(\"model_full.pt\", weights_only=False) # More than just the parameters\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check inference\n",
    "new_model.eval()\n",
    "\n",
    "# Call the model with input to get a prediction\n",
    "output = new_model(sample_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading a Checkpoint\n",
    "A model checkpoint is a way to save parameters as a snapshot in a point in time. \n",
    "\n",
    "This is helpful to continue a long training job that may have failed at some point or to give multiple models as options to use from a training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a checkpoint\n",
    "import torch\n",
    "\n",
    "# dummy epoch and loss\n",
    "epoch = 5\n",
    "loss = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a checkpoint\n",
    "torch.save({'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss}, \n",
    "            f'{epoch}_checkpoint.tar') # .tar file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint\n",
    "# Initialize the Model as we have before. NOTE: also optimizer in our case\n",
    "from fake_net import FakeNet\n",
    "\n",
    "# Initialize and use the model\n",
    "model = FakeNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model as a checkpoint\n",
    "import torch\n",
    "\n",
    "# Load the tar file\n",
    "checkpoint = torch.load(f\"{epoch}_checkpoint.tar\", weights_only='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the checkpoint info\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parameters to our model\n",
    "model.load_state_dict(checkpoint['model_state_dict']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimizer\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the loss and the epoch. NOTE that we could have save other information here as well\n",
    "loss = checkpoint['loss']\n",
    "epoch = checkpoint['epoch']\n",
    "print(loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "model.eval()\n",
    "output = model(sample_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Checkpoints to Training\n",
    "Its good practice to include checkpoints as part of your training loop.\n",
    "\n",
    "How you save checkpoints is up to you. ie: every so often, every epoch, every epoch which improves on loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets include a checkpoint in our training loop that saves a checkpoint every 2 epochs\n",
    "# Train a fake model\n",
    "N_EPOCHS = 10\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    ######### Save a checkpoint every 2 epochs\n",
    "    if epoch % 2 == 0:\n",
    "        torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss}, \n",
    "                f'training_checkpoint_{epoch}.tar')\n",
    "\n",
    "# Save the final checkpoint after the last epoch\n",
    "torch.save({\n",
    "    'epoch': N_EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss\n",
    "}, 'training_checkpoint_final.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the checkpoints\n",
    "!ls -l training_checkpoint*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: We can now load any of these checkpoints to either continue training from that point in time or run inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmstarting\n",
    "Warmstarting is where we initialize a new model to train from trained parameters of a previously trained model.\n",
    "\n",
    "This is helpful in Transfer Learning which is covered in more detail later.\n",
    "\n",
    "With warmstarting we can also initialize only certain layers of a previously trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Fake Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FakeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FakeNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.batch_norm = nn.BatchNorm1d(50) \n",
    "        self.fc2 = nn.Linear(50, 1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))              \n",
    "        x = self.batch_norm(x)               \n",
    "        x = self.fc2(x)                      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new model\n",
    "new_model = FakeNet()\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the parameters\n",
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our very first trained model parameters into the new one\n",
    "new_model.load_state_dict(torch.load('model_state_dict.pt'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the new parameters\n",
    "print(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now take the parameters we just added into this model and train it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Across Devices\n",
    "PyTorch supports multiple different devices such as CPU and GPUs.\n",
    "\n",
    "Its common practice to train on a GPU for speed but do inference on a CPU for cost for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model on CPU that was saved on GPU\n",
    "import torch\n",
    "\n",
    "model = torch.load('model_state_dict.pt', map_location='cpu', weights_only=True) # Using map_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU to GPU and GPU to GPU\n",
    "model = torch.load('model_state_dict.pt', map_location='cuda:0', weights_only=True) # Using map_location to the GPU device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to above we must also put the model on the GPU:\n",
    "\n",
    "```py\n",
    "model.to('cuda')\n",
    "```\n",
    "\n",
    "As well as the inputs for inference.\n",
    "```py\n",
    "model.eval()\n",
    "outputs = model(sample_input.to('cuda'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember our is_available() function to find the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
